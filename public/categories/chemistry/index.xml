<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Chemistry on AmanR</title>
    <link>http://example.org/categories/chemistry/</link>
    <description>Recent content in Chemistry on AmanR</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 May 2021 20:12:26 +0530</lastBuildDate><atom:link href="http://example.org/categories/chemistry/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ml</title>
      <link>http://example.org/posts/ml/</link>
      <pubDate>Thu, 20 May 2021 20:12:26 +0530</pubDate>
      
      <guid>http://example.org/posts/ml/</guid>
      <description>AML random notes Introduction to Machine Learning Week  Loss functions    L1 vs L2
L1 better for outliers, but L2 easier to solve. L1 is useful when data is corrupt with outliers. Consider L1 vs L2 as mean vs median (think about their minimas). L1 has constant gradient.
Cons : Cases where both give undesirable predictions.
  Huber loss Linear above delta, quadratic below it. $\delta$ is critical hyperparameter.</description>
    </item>
    
  </channel>
</rss>
