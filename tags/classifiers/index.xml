<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Classifiers on AmanR</title><link>http://github.com/SarcasticNastik/website-template/tags/classifiers/</link><description>Recent content in Classifiers on AmanR</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 20 May 2021 20:12:26 +0530</lastBuildDate><atom:link href="http://github.com/SarcasticNastik/website-template/tags/classifiers/index.xml" rel="self" type="application/rss+xml"/><item><title>Ml</title><link>http://github.com/SarcasticNastik/website-template/posts/ml/</link><pubDate>Thu, 20 May 2021 20:12:26 +0530</pubDate><guid>http://github.com/SarcasticNastik/website-template/posts/ml/</guid><description>AML random notes Introduction to Machine Learning Week Loss functions L1 vs L2
L1 better for outliers, but L2 easier to solve. L1 is useful when data is corrupt with outliers. Consider L1 vs L2 as mean vs median (think about their minimas). L1 has constant gradient.
Cons : Cases where both give undesirable predictions.
Huber loss Linear above delta, quadratic below it. $\delta$ is critical hyperparameter.</description></item></channel></rss>