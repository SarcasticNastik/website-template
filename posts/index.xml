<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on AmanR</title><link>http://example.org/posts/</link><description>Recent content in Posts on AmanR</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 20 May 2021 20:12:26 +0530</lastBuildDate><atom:link href="http://example.org/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Ml</title><link>http://example.org/posts/ml/</link><pubDate>Thu, 20 May 2021 20:12:26 +0530</pubDate><guid>http://example.org/posts/ml/</guid><description>AML random notes Introduction to Machine Learning Week Loss functions L1 vs L2
L1 better for outliers, but L2 easier to solve. L1 is useful when data is corrupt with outliers. Consider L1 vs L2 as mean vs median (think about their minimas). L1 has constant gradient.
Cons : Cases where both give undesirable predictions.
Huber loss Linear above delta, quadratic below it. $\delta$ is critical hyperparameter.</description></item><item><title>Pnc</title><link>http://example.org/posts/pnc/</link><pubDate>Thu, 20 May 2021 19:05:35 +0530</pubDate><guid>http://example.org/posts/pnc/</guid><description>A First Course In Probability tags: Mathematics Probability Chapter 1, 2, 3 - Basic stuff üìù Theoretical Takeaways Considering question 80, if a particular formulation gets stuck again, try to backtrack if the paradigm seems correct. Try to find different approaches for finding the same answer üòâ. Considering the Conditional Probability, keep in mind to first cascade the conditional probability i.e. consider every sub-event to create sub-linear dependencies. Go on proving afterwards whether the events are independent or not.</description></item></channel></rss>